(x)
##
신용카드 사기, 사이버 침입 등과 같은 이상값은 데이터를 학습하는데 있어 불필요한 요소이다. 이상치 탐지를 위한 연구 방법론 중 기계학습은 AI를 활용해 데이터를 학습시키는 지도학습의 방법으로 뛰어난 이상치 탐지 기법 중 하나이다. 이상치를 탐지하기 위한 적용 분야로는 사이버 보안, 의학 분야, 금융 분야 등 다양한 분야가 있으며 본 연구에서는 금융 분야에 관심을 두고 연구하기로 한다.

##



**IT 등 정보기술의 발달로 많은 나라가 현금의 사용을 줄이고 신용 카드를 이용해 사용하고 있다. 우리나라도 2022년 한국은행이 발표한 '2021년 경제주체별 현금사용행태 조사'*(3년 마다 진행되는 조사)결과에 따르면, 현금 사용 비중은 2015년 38.8%에서 2018년 32.1%로 하락한 뒤 이번에 처음으로 20%대로 낮아졌다. 반면 신용·체크카드 비중은 2015년 37.4%, 2018년 52.0%, 2021년 58.3%로 증가했다.(출처: 한국은행-2021년 경제주체별 현금사용행태 조사 결과)

현금의 유동성은 감소하고 가상 화폐, 카드 등의 거래가 커지고 있는 셈이다. 신용카드의 사용은 소비자의 거래 편리성을 제공하고 소비 패턴을 분석하는데 있어 용이하지만, 카드 사기 거래를 야기시킨다. 이에 카드 발급기관 및 소비자는 카드의 사기거래를 파악하지 못하게 되면 커다란 금융 피해를 야기하게 된다. **




(x)
##

기존의 사기거래 탐지 기법으로 소개된 기계학습 방법론들은 사기 거래마다 독립성을 가정해 데이터를 학습한다. 이는 전체의 데이터 중 사기 거래를 파악하는 데 있어 우수한 성능을 보일 수 있다. 하지만, **전체 사기 거래 여부보다는 개인의 사기 거래 해당 여부 파악이 중요하다. ~~ 고객과 카드사의 입장에서는 전체의 거래에서의 사기 거래 여부도 중요하지만, 고객별 사기거래 여부를 파악하는 것이 중요하다.**
 
 (# 포스터)
 하지만, 기존의 사기거래 탐지 기법은 고액의 거래를 분석하며 복잡하고 적은 금액의 사기 거래 탐지는 잘 하지 못한다. 이에, 본 연구에서는 그래프를 이용해 사기 거래 탐지에 연구하고자 한다.
##


 
 
 
**
이러한 신용카드 사기거래와 같은 이상치는 일반적인 행동으로 정의된 개념에 부합하지 않는 패턴을 말한다.[@chandola2009anomaly] 그림 2은 간단한 이차원 데이터 집합에서의 이상치를 보여준다. 데이터에는 대부분의 관측치가 이 두 영역에 위치해 있는 정상 영역 N1과 N2에 있고, 이러한 영역으로부터 충분히 떨어진 점  o1과 o2, 그리고 영역 O3에 있는 점들은 이상치로 정의한다.

이상치 탐지를 위한 기존 연구로는 Classification, Clustering,  Nearest Neighbo Based, Statistical 등의 방법이 있다. 기존 연구 방법론을 활용한 사기 거래 탐지 연구는 복잡한 사기 패턴을 포착하지 못한다. 이에 본 연구에서는 그래프를 활용하여 사기 거래의 이상치를 탐지하고자 한다.

그래프 기반 분석 방법으로는 이분형 그래프와 삼분형 그래프가 있다. 이분형 그래프는 개별 거래의 이상치 탐지에 어려움이 있다. 삼분형은 개별 수준의 분석이 용이하지만 거래 시간이 오래 걸리고 복잡하다는 단점이 있다.

이에 본 연구에서는 거래 간 시간 차이를 이용해 그래프의 연결성을 향상시키는 방법을 제안하고자 한다.
이 방법을 통해 확인하고 싶은 내용은 다음과 같다.

1. 비 그래프 기반 연구 방법에 비해 개별 거래 파악이 용이한가?

2. 그래프 기반 연구 방법에 비해 간단한가? 


**
Related Studies

그래프 관련 기반 사기 거래 탐지 연구를 위해서 먼저, 그래프에 대해 알아보자.

- Graph $G = (V,E)$ with $N$ nodes $i \in v$ and $|E|$ edges $(i,j) \in E$

- $A \in \mathbb{R}^{N \times N}$: adjacency matrix of G

- $X \in \mathbb{R}^{N \times P}$ is matrix of node features

- 그래프 G는 (V,E)로 이루어져 있고, V는 node, E는 edge의 집합이다. 각 노드와 에지는 특징을 가지고 있고 간단하게 아래와 같은 그림으로 나타낼 수 있다.

![그래프 간단한 특성? 사진]()


**
본 연구에서 사용하는 데이터는 불균형이 심한 데이터이다. 원 데이터의 사기거래 비율은 약 0.00573이다. 사기 거래의 불균형이 심하므로 해당 실험에서는 전체 데이터의 사기 거래 비율, 훈련, 검증 데이터의 사기 거래 비율을 조정하여 실험하고자 한다. 이는, 실제 사기거래가 이루어지는 데이터를 활용하고자 할 때 해당 비율에 따른 연구 검증 효과를 확인 할 수 있을 것이다.

데이터 사기 거래 비율이 약 0.00573인 경우를 시작으로 점점 비율을 높여가며 실험을 진행하였다.

실험은 이상치 탐지 기법으로 잘 알려진 pyod 기법, autogluon, 제안된 연구 방법을 함께 비교해 보았다.


실험 1
원 데이터(0.00573/0.00573/0.00573)
----> proposed 다시 확인
df02 (0.2/0.2/0.2)
데이터 불균형이 심해서 언더샘플링 하는게 좋당

실험 3
df50 (0.5/0.5/0.5)

실험 4
(0.1/0.5/0.05)
-인덱스 겹침?

---> 이거 인덱스 안겹치는 것도 해봐도 좋을듯! 
(왜냐하면 데이터가 너무 없게 되면 인덱스 겹치는 것도 조금이나마, 개별 거래 생각하면....... 학습시키는게 나쁘지 않을거같은 생ㅇ각.) 

실험 5
(0.1/0.5/0.01)
인덱스 안겹침


df50_tr, df50_test
전체 df_fruad 비율: 0.5
proposed: 0.97~
autogluon:0.960783(XGBoost)
pyod: 0.736782(COPOD)

df50_tr, df005_test (인덱스가 겹치는게 있음)
전체 df_fruad 비율: 0.1
proposed: 0.980860
autogluon: 0.968886(CatBoost)
pyod: 0.753193(MCD)

df50_tr, df001_test (인덱스 안겹침)
전체 df_fraud 비율: 0.1
proposed: 0.982845
autoluon:0.971228(LightGBMXT)
pyod: 0.822422(IForest)


------



240315이후

기존의 사기거래 여부를 파악하기 위해서 다양한 연구가 진행되어 왔다. 해당 연구는 사기 거래 파악을 위해 거래 금액에 집중하여 학습하고 분석하였다. 이러한 가정은 각 거래를 독립적으로 보므로 개인의 사기거래 여부 파악이 어렵다. 
각 거래가 독립적이지 않다면, 어떻게 될지 간단하게 생각해보자. 소비자가 카드를 분실하는 경우, 분실 여부를 알기 전까지 카드는 사기 거래에 사용된다. 본 연구에서 사용할 데이터를 고객별로 그룹핑하고, 시간에 따라 시각화 해 본 결과 아래와 같이 나타난다.

#data description

본 연구에서 제시한 방법으로 사기 거래를 파악하기 위해, 먼저 사기거래 관련 데이터셋을 준비한다. 데이터는 케글, 신용거래, 금융거래소 등의 사이트에서 얻을 수 있다. 다만 이러한 데이터들은 고객의 개인정보 보호를 위해 가명처리 되어 데이터 분석을 위해 제공된 데이터이다. 본 연구의 방법을 사용하기 위해서는 고유 index(본 연구에서는 cc_num을 사용하였다.), time, node(본 연구에서는 amt를 사용하였다.)가 필요하다. 앞서 말한 바와 같이 다른 데이터들은 index 혹은 time이 없는 데이터로 본 연구에서는 사용이 어렵다.

앞서 말한 바와 같이, 다른 데이터 셋들은 사용이 어려우므로, 본 연구에서는 2013년 9월 한 달 동안 수집된 유럽 카드 소지자의 신용카드 거래 기록이 포함된 데이터 셋을 이용한다.

# 실험

신용카드 사기거래 데이터는 불균형이 심한 데이터로 이에 데데이터.
실험 1

df1은 사기거래 비율이 0.00573으로 불균형이 심한 df이다.
원 데이터(0.00573/0.00573/0.00573)
----> proposed 다시 확인
df02 (0.2/0.2/0.2)
데이터 불균형이 심해서 언더샘플링 하는게 좋당


실험비교를 어떻게 하는게 좋을까?
1. 원본 vs df02
- 원본데이터에서는 proposed 방법이 되지 않는데.. 어떻게 하면 좋을지 고민 ㅠ
- pyod와 autogloun은 작용됨
- autogluon auc값은 0.955
- proposed는 이전에 한 것은 auc랑 accuracy말고 0 값이 나와서 다시 돌려보니까 안돌아감............. (혹시 몰라서 다시 한번 걍 코드 풀어서 하늕붕.. )

---> 해당 결과를 통해 데이터불균형이 심해서 버려야 된다는 내용을 써줄까?
그렇게 되면, 결국 df02가 이기게 뎀

2. 

